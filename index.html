<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Text-to-Image Synthesis with Dynamic Masking</title>
    <style>
        * {
            border: 0;
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            padding: 20px;
            margin: 0 auto;
            padding: 0;
        }

        .navbar {
            background-color: #333;
            overflow: hidden;
            display: flex;
            justify-content: center; /* Center the content */
            align-items: center;
            width: 100%;
            padding: 20px 0; /* Add vertical padding */
        }

        .navbar-center {
            text-align: center; /* Center-align the inner content */
        }

        .navbar-title {
            color: white;
            font-size: xx-large;
            margin-bottom: 10px; /* Space between title and names */
        }

        .navbar-info {
            color: white;
            margin-bottom: 10px; /* Space between names and GitHub button */
        }

        .github-btn {
            background-color: #4CAF50;
            color: white;
            padding: 10px 15px;
            text-decoration: none;
            border-radius: 5px;
            font-size: 14px;
        }

        .github-btn:hover {
            background-color: #45a049;
        }

        .container {
            max-width: 1000px;
            margin: 0 auto;
        }

        h1, h2 {
            margin-bottom: 20px;
            margin-top: 20px;
        }

        a {
            display: inline-block;
            margin-top: 20px;
            padding: 10px 15px;
            background-color: #007BFF;
            color: #FFF;
            text-decoration: none;
            border-radius: 5px;
        }

        .dropdown {
            margin: 20px 0;
        }

        .dropdown select {
            display: block;
            width: 100%;
            padding: 10px;
            font-size: 16px;
            border: 1px solid #ddd;
            border-radius: 5px;
        }

        .output {
            margin-top: 20px;
            padding: 15px;
            background-color: #f0f0f0;
            border: 1px solid #ddd;
            border-radius: 5px;
            margin-bottom: 5em;
        }
        .team-contributions {
            margin-top: 20px;
        }

        .team-contributions h2 {
            margin-bottom: 10px;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 10px;
            margin-bottom: 40px;
        }

        table, th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }

        th {
            background-color: #f2f2f2;
        }
        .related-works {
            text-align: justify;
        }

        .related-works p {
            display: inline-block;
            width: 100%;
        }

        .experiment-image {
            max-width: 200px; /* Adjust the maximum width as needed */
            height: auto;
        }

        .architecture-diagram {
            margin-top: 20px;
            text-align: center;
        }

        .output-container {
            display: flex;
            align-items: center;
            margin-bottom: 2em;
            margin-top: 2em;
        }
        .output-image {
            max-width: 300px;
            height: auto;
            margin: 0 10px;
        }
        .output-text {
            font-size: 18px;
            margin: 0 10px;
        }

    </style>
</head>

<body>
    <nav class="navbar">
        <div class="navbar-center">
            <div class="navbar-title">
                Text-to-Image Synthesis with Dynamic Masking
            </div>
            <div class="navbar-info">
                Kshitij Pathania, Brandon Colbert, Mathan Mahendran
            </div>
            <!-- Replace the href with your repository link -->
            <a href="https://github.gatech.edu/CS-6476-Diffusion-Model/Masked-Diffusion-Image-Synthesis" target="_blank" class="github-btn">GitHub Repository</a>
        </div>
    </nav>
    
    
    <div class="container">
        <h1>Text-to-Image Synthesis Project</h1>

        <!-- Problem Statement Section -->
        <h2>Problem Statement</h2>
        <div class="related-works">
        <p>With current text-to-image models such Stable Diffusion, images are generated entirely based on a specified prompt. While this is an amazing capability by itself, the functionality could be extended to allow further control based on supplied images, such as correlating a prompt with a desired background. </p>
        <p>As such, we aim to develop an advanced text-to-image synthesis method by seamlessly integrating a dynamic masking technique with diffusion models. This approach seeks to enhance the accuracy of drawing subjects from input text onto an image while ensuring their seamless blending with the original image's visual features and background.</p>

        For example consider the image generated from stable diffusion model for the prompt "A dog with Sunglasses" when hyperparameter determining noising strength is set to 0.9 for stable diffusion model. 
        <div class="output-container">
            <img src="./static/reference.jpeg" alt="Reference Dog with Sunglasses" class="output-image"> 
            <div class="output-text">
                <p><b> + Prompt: "A dog with Sunglasses" = </b></p>
            </div>
            <img src="./static/dog-sunglasses-baseline.png" alt="Baseline Dog with Sunglasses" class="output-image" style="max-height: 13em;">
        </div>
        <p style="text-align: center; margin-bottom: 2em;">Figure 1: Image generated from stable diffusion model for the prompt "A dog with Sunglasses"</p>
        <p>
            The model generates the image of a dog with sunglasses but the background is <b>not coherent with the reference image</b> (in above example see the background is very different from the reference image). Our model will mask the less attended regions in the image and replace them with the original image to preserve the context and orginal image features. This will result in a more coherent image with the prompt and reference image.
        </p>
        

        <p>
            Such a model would be invaluable in contexts where there is a need to integrate objects into predefined backgrounds. For instance, it could be used in illustrations in digital art, where we want to integrate 
            new elements into a scene predefined by the artists, allowing them to enhance their creativity, without compromising the scene's integrity. It could also prove to be useful in advertising campaigns by companies by allowing them to showcase products in a variety of environment settings, 
            they could potentially do this without the need for a traditional photo shoots, thereby reducing high costs, and logistical complexity while makeing advertisements.
        </p>

        </div>

        <h2>Related Works</h2>
        <div class="related-works">
            <p>The research landscape in high-resolution image synthesis and denoising with latent diffusion models encompasses several key contributions. At the forefront is the original paper on stable diffusion models for image synthesis, titled "High-Resolution Image Synthesis with Latent Diffusion Models" [1]. Building upon this foundation, "Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models" introduces attention mechanisms to ensure adequate focus on subjects within prompts, enhancing the quality of generated images [2].</p>
            <p>Meanwhile, "SinDDM: A Single Image Denoising Diffusion Model" proposes a user-guided approach to image denoising, leveraging regions of interest (ROIs) selected by users and inspired by prior hierarchical text-conditional image generation techniques [3]. In the realm of multimodal image inpainting, the "Uni-paint Framework" offers a unified solution utilizing pretrained diffusion models, albeit with the caveat of computational expense due to fine-tuning requirements [6]. Additionally, "SAT: Self-Attention Control for Diffusion Models Training and Improving Sample Quality of Diffusion Models Using Self-Attention Guidance" presents a method for self-attention control, enhancing image quality by selectively blurring unattended areas and iteratively denoising the image [4].</p>
            <p>These approaches collectively advance the understanding and application of diffusion models in image synthesis and denoising tasks, offering diverse strategies for improving sample quality and enhancing user control in the generation process. Moreover, efforts to implement Stable Diffusion from scratch in Python [5] contribute to the accessibility and reproducibility of these advancements, fostering further research and development in the field.</p>
        </div>
        <!-- Methods/Approach Section -->
        <h2>Methods/Approach</h2>
        <div class="related-works">
        <p>Our approach involves integrating dynamic masking techniques with diffusion models for text-to-image synthesis. We leverage cumulative attention scores derived from the text prompt to mask specific areas of the image during the synthesis process. This allows us to preserve contextual information while generating visually coherent images.</p>
        <p> More specifically for our approach, we utilize an image generation pipeline that references the following parameters. A prompt to define the subject details of the output and an input image used to create the background. The constants for noise strength, classifier-free guidance scale, and inference step count all play a role in determining the quality and level of presence the background will have in the resulting image. A seed specifier is also used to allow a result to be replicated exactly.
            To construct the desired image, the prompt must first be analyzed. For this, spacy, a language processing library, is used to extract words correlating with the prompt's subject matter. Next, the input image is encoded and its latents with and without noise at the specified strength are calculated for certain timesteps. Now during the denoising step, for the selected timestamp, we calculate the cumulative attention map for the various subjects present in the conditional prompt. Based on these attention values, we construct our dynamic mask which identifies whether a point in the latent vector will be swapped with the noised original latent vector at that timestep. We repeat this process for all the selected timesteps. 
        </p>
        </div>
        <!-- Architecture Diagram -->
        <div class="architecture-diagram">
            <img src="./static/Stable-diffusion.png" alt="Architecture Diagram" style="max-width: 70%; height: auto; margin-top: 20px;">
            <p>Figure 2: Architecture Diagram of Text-to-Image Synthesis with Dynamic Masking</p>
        </div>    
        <!-- Experiments / Results Section -->
        <h2>Experiments / Results</h2>
        <div class="related-works">
        <p>For our experiments, we conducted text-to-image synthesis using both a baseline stable diffusion model and our masked model, with the aim of generating visually coherent images from textual prompts. We utilized the <b>Common Objects in Context (COCO) dataset</b> to create our database of reference images. The baseline model was configured with hyperparameters optimized for stability and quality, including a configuration scale of 8, a sampler of "ddpm," 50 inference steps, a seed value of 42, and a diffusion strength of 0.9. Among 50 timestamps, we selected the 5th, 10th and 15th timestamp for dynamic mask calculation and superimposed it over the predicted latent vector for those selected timestamps. 
            Dynamic map calculation was performed by calculating the attention scores for each word in the prompt and then normalizing the scores to obtain the mask. The threshold for the mask was set to 0.0002 and the regions with attention scores above the threshold were masked. In short we identify the less attended regions in the image and replace them with the orginal image to preserve the context.
            We evaluated our models across various prompts, ranging from simple descriptions like "Dog Sitting" to more complex scenarios such as "A Lion with Sunglasses." Each experiment involved synthesizing images based on the prompt using both the baseline model and our masked model. Additionally, we utilized an image of a "Dog Sitting in Grass" as a reference image to provide context for the synthesis process. 
            In our results, we observed the effectiveness of our masked model in preserving context and blending images compared to the baseline model. The structured integration of dynamic masking techniques with diffusion models allowed for enhanced accuracy in drawing subjects from input text onto images, ensuring seamless blending with the original image's visual features and background. Through meticulous experimentation and optimization, we aimed to validate the effectiveness of our approach in improving sample quality and providing greater user control in the generation process.
            Some of the qualitative results are shown below.
        </p>
        </div>

        <div class="output">
            <table>
                <tr>
                    <th>Prompt</th>
                    <th>Reference Image</th>
                    <th>Baseline Model: Stable Diffusion</th>
                    <th>Our Model</th>
                </tr>
                <tr>
                    <td>A dog with Sunglasses</td>
                    <td><img src="./static/reference.jpeg" alt="Reference Dog with Sunglasses" class="experiment-image"></td>
                    <td>
                        <img src="./static/dog-sunglasses-baseline.png" alt="Baseline Dog with Sunglasses" class="experiment-image">
                        <p>FID Score: 30.9550</p>
                    </td>
                    <td>
                        <img src="./static/dog-sunglasses-our.png" alt="Our Model Dog with Sunglasses" class="experiment-image">
                        <p>FID Score: 3.1471</p>
                    </td>
                </tr>
                <tr>
                    <td>A lion with Sunglasses</td>
                    <td><img src="./static/reference.jpeg" alt="Reference Dog with Sunglasses" class="experiment-image"></td>
                    <td>
                        <img src="./static/lion-sunglasses-baseline.png" alt="Baseline Dog with Sunglasses" class="experiment-image">
                        <p>FID Score: 5.7031</p>
                    </td>
                    <td>
                        <img src="./static/lion-sunglasses-our.png" alt="Our Model Dog with Sunglasses" class="experiment-image">
                        <p>FID Score: 1.7544</p>
                    </td>
                </tr>
                <tr>
                    <td>Horse running.</td>
                    <td><img src="./static/reference.jpeg" alt="Reference Dog with Sunglasses" class="experiment-image"></td>
                    <td>
                        <img src="./static/horse-running-baseline.png" alt="Baseline Dog with Sunglasses" class="experiment-image">
                        <p>FID Score: 40.7026</p>
                    </td>
                    <td>
                        <img src="./static/horse-running-our.png" alt="Our Model Dog with Sunglasses" class="experiment-image">
                        <p>FID Score: 5.7273</p>
                    </td>
                </tr>
            </table>

            <p> In the above table FID stands for Frechet Inception Distance, and is used to evaluate the quality of generated images. A lower FID score indicates a closer match to the original image. </p>
            
        </div>

        <!-- What’s Next Section -->
        <h2>What’s Next</h2>
        <p>Our plan until the final project due date involves refining our masking technique, exploring additional datasets for evaluation, and conducting thorough experiments to validate our approach's effectiveness. We will also compare our results against baselines to assess performance improvements.</p>
        <!-- Task list indicating each step planned and anticipated completion date -->
        <table>
            <tr>
                <th>Task</th>
                <th>Anticipated Completion Date</th>
            </tr>
            <tr>
                <td>Refine and Improve masking technique to incorporate complex scenerios</td>
                <td>April 10, 2024</td>
            </tr>
            <tr>
                <td>Ablation Studies to identify better hyperparameters</td>
                <td>April 15, 2024</td>
            </tr>
            <tr>
                <td>Conduct further experiments</td>
                <td>April 20, 2024</td>
            </tr>
        </table>

        <!-- Team Contributions Section -->
        <div class="team-contributions">
            <h2>Team Contributions</h2>
            <table>
                <tr>
                    <th>Team Member</th>
                    <th>Contributions</th>
                </tr>
                <tr>
                    <td>Kshitij Pathania</td>
                    <td>Implemented dynamic masking technique, conducted experiments</td>
                </tr>
                <tr>
                    <td>Brandon Colbert</td>
                    <td>Research on related works, assisted in experimentation</td>
                </tr>
                <tr>
                    <td>Mathan Mahendran</td>
                    <td>Dataset exploration, documentation</td>
                </tr>
            </table>
        </div>
       
        <h3>References</h3>
        <ol>
        <li>
            <p>R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, "High-Resolution Image Synthesis with Latent Diffusion Models," 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Jun. 2022. DOI: 10.1109/cvpr52688.2022.01042</p>
        </li>
        <li>
            <p>H. Chefer, Y. Alaluf, Y. Vinker, L. Wolf, and D. Cohen-Or, "Attend-and-Excite: Attention-based semantic guidance for text-to-image diffusion models," ACM Transactions on Graphics, vol. 42, no. 4, pp. 1–10, Jul. 2023. DOI: 10.1145/3592116</p>
        </li>
        <li>
            <p>K. He et al., "Masked autoencoders are scalable vision learners," 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Jun. 2022. DOI: 10.1109/cvpr52688.2022.01553</p>
        </li>
        <li>
            <p>U. Jamil, "HKPROJ/Pytorch-stable-diffusion: Stable diffusion implemented from scratch in PyTorch," GitHub, https://github.com/hkproj/pytorch-stable-diffusion (accessed Feb. 21, 2024).</p>
        </li>
        <li>
            <p>"CompVis/stable-diffusion · hugging face," CompVis/stable-diffusion · Hugging Face, https://huggingface.co/CompVis/stable-diffusion (accessed Feb. 21, 2024).</p>
        </li>
        <li>
            <p>Create high-quality images with stable diffusion models and deploy ..., https://aws.amazon.com/blogs/machine-learning/create-high-quality-images-with-stable-diffusion-models-and-deploy-them-cost-efficiently-with-amazon-sagemaker/ (accessed Feb. 22, 2024).</p>
        </li>
        </ol>

    </div>
</body>

</html>
