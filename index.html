<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Text-to-Image Synthesis with Dynamic Masking</title>
    <style>
        * {
            border: 0;
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            padding: 20px;
            margin: 0 auto;
            padding: 0;
        }

        .navbar {
            background-color: #333;
            overflow: hidden;
            display: flex;
            justify-content: center; /* Center the content */
            align-items: center;
            width: 100%;
            padding: 20px 0; /* Add vertical padding */
        }

        .navbar-center {
            text-align: center; /* Center-align the inner content */
        }

        .navbar-title {
            color: white;
            font-size: xx-large;
            margin-bottom: 10px; /* Space between title and names */
        }

        .navbar-info {
            color: white;
            margin-bottom: 10px; /* Space between names and GitHub button */
        }

        .github-btn {
            background-color: #4CAF50;
            color: white;
            padding: 10px 15px;
            text-decoration: none;
            border-radius: 5px;
            font-size: 14px;
        }

        .github-btn:hover {
            background-color: #45a049;
        }

        .container {
            max-width: 1000px;
            margin: 0 auto;
        }

        h1, h2 {
            margin-bottom: 20px;
            margin-top: 20px;
        }

        a {
            display: inline-block;
            margin-top: 20px;
            padding: 10px 15px;
            background-color: #007BFF;
            color: #FFF;
            text-decoration: none;
            border-radius: 5px;
        }

        .dropdown {
            margin: 20px 0;
        }

        .dropdown select {
            display: block;
            width: 100%;
            padding: 10px;
            font-size: 16px;
            border: 1px solid #ddd;
            border-radius: 5px;
        }

        .output {
            margin-top: 20px;
            padding: 15px;
            background-color: #f0f0f0;
            border: 1px solid #ddd;
            border-radius: 5px;
            margin-bottom: 5em;
        }
        .team-contributions {
            margin-top: 20px;
        }

        .team-contributions h2 {
            margin-bottom: 10px;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 10px;
            margin-bottom: 40px;
        }

        table, th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }

        th {
            background-color: #f2f2f2;
        }
        .related-works {
            text-align: justify;
        }

        .related-works p {
            display: inline-block;
            width: 100%;
        }

        .experiment-image {
            max-width: 200px; /* Adjust the maximum width as needed */
            height: auto;
        }

        .architecture-diagram {
            margin-top: 20px;
            text-align: center;
        }

        .output-container {
            display: flex;
            align-items: center;
            margin-bottom: 2em;
            margin-top: 2em;
        }
        .output-image {
            max-width: 300px;
            height: auto;
            margin: 0 10px;
        }
        .output-text {
            font-size: 18px;
            margin: 0 10px;
        }

    </style>
</head>

<body>
    <nav class="navbar">
        <div class="navbar-center">
            <div class="navbar-title">
                Text-to-Image Synthesis with Dynamic Masking
            </div>
            <div class="navbar-info">
                Kshitij Pathania, Brandon Colbert, Mathan Mahendran
            </div>
            <!-- Replace the href with your repository link -->
            <a href="https://github.gatech.edu/CS-6476-Diffusion-Model/Masked-Diffusion-Image-Synthesis" target="_blank" class="github-btn">GitHub Repository</a>
        </div>
    </nav>
    
    
    <div class="container">
        <h1>Text-to-Image Synthesis Project</h1>

        <!-- Problem Statement Section -->
        <h2>Problem Statement</h2>
        <div class="related-works">
        <p>With current text-to-image models such Stable Diffusion, images are generated entirely based on a specified prompt. While this is an amazing capability by itself, the functionality could be extended to allow further control based on supplied images, such as correlating a prompt with a desired background. </p>
        <p>As such, we aim to develop an advanced text-to-image synthesis method by seamlessly integrating a dynamic masking technique with diffusion models. This approach seeks to enhance the accuracy of drawing subjects from input text onto an image while ensuring their seamless blending with the original image's visual features and background.</p>

        For example consider the image generated from stable diffusion model for the prompt "A dog with Sunglasses" when hyperparameter determining noising strength is set to 0.9 for stable diffusion model. 
        <div class="output-container">
            <img src="./static/reference.jpeg" alt="Reference Dog with Sunglasses" class="output-image"> 
            <div class="output-text">
                <p><b> + Prompt: "A dog with Sunglasses" = </b></p>
            </div>
            <img src="./static/dog-sunglasses-baseline.png" alt="Baseline Dog with Sunglasses" class="output-image" style="max-height: 13em;">
        </div>
        <p style="text-align: center; margin-bottom: 2em;">Figure 1: Image generated from stable diffusion model for the prompt "A dog with Sunglasses"</p>
        <p>
            The model generates the image of a dog with sunglasses but the background is <b>not coherent with the reference image</b> (in above example see the background is very different from the reference image). Our model will mask the less attended regions in the image and replace them with the original image to preserve the context and orginal image features. This will result in a more coherent image with the prompt and reference image.
        </p>
        

        <p>
            Such a model would be invaluable in contexts where there is a need to integrate objects into predefined backgrounds. For instance, it could be used in illustrations in digital art, where we want to integrate 
            new elements into a scene predefined by the artists, allowing them to enhance their creativity, without compromising the scene's integrity. It could also prove to be useful in advertising campaigns by companies by allowing them to showcase products in a variety of environment settings, 
            they could potentially do this without the need for a traditional photo shoots, thereby reducing high costs, and logistical complexity while makeing advertisements.
        </p>

        </div>

        <h2>Related Works</h2>
        <div class="related-works">
            <p>The research landscape in high-resolution image synthesis and denoising with latent diffusion models encompasses several key contributions. At the forefront is the original paper on stable diffusion models for image synthesis, titled "High-Resolution Image Synthesis with Latent Diffusion Models" [1]. Building upon this foundation, "Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models" introduces attention mechanisms to ensure adequate focus on subjects within prompts, enhancing the quality of generated images [2].</p>
            <p>Meanwhile, "SinDDM: A Single Image Denoising Diffusion Model" proposes a user-guided approach to image denoising, leveraging regions of interest (ROIs) selected by users and inspired by prior hierarchical text-conditional image generation techniques [3]. In the realm of multimodal image inpainting, the "Uni-paint Framework" offers a unified solution utilizing pretrained diffusion models, albeit with the caveat of computational expense due to fine-tuning requirements [6]. Additionally, "SAT: Self-Attention Control for Diffusion Models Training and Improving Sample Quality of Diffusion Models Using Self-Attention Guidance" presents a method for self-attention control, enhancing image quality by selectively blurring unattended areas and iteratively denoising the image [4].</p>
            <p>These approaches collectively advance the understanding and application of diffusion models in image synthesis and denoising tasks, offering diverse strategies for improving sample quality and enhancing user control in the generation process. Moreover, efforts to implement Stable Diffusion from scratch in Python [5] contribute to the accessibility and reproducibility of these advancements, fostering further research and development in the field.</p>
        </div>
        <!-- Methods/Approach Section -->
        <h2>Methods/Approach</h2>
        <div class="related-works">
        <p>Our approach involves integrating dynamic masking techniques with diffusion models for text-to-image synthesis. We leverage cumulative attention scores derived from the text prompt to mask specific areas of the image during the synthesis process. This allows us to preserve contextual information while generating visually coherent images.</p>
        <p> More specifically for our approach, we utilize an image generation pipeline that references the following parameters. A prompt to define the subject details of the output and an input image used to create the background. The constants for noise strength, classifier-free guidance scale, and inference step count all play a role in determining the quality and level of presence the background will have in the resulting image. A seed specifier is also used to allow a result to be replicated exactly.
            To construct the desired image, the prompt must first be analyzed. For this, spacy, a language processing library, is used to extract words correlating with the prompt's subject matter. Next, the input image is encoded and its latents with and without noise at the specified strength are calculated for certain timesteps. Now during the denoising step, for the selected timestamp, we calculate the cumulative attention map for the various subjects present in the conditional prompt. Based on these attention values, we construct our dynamic mask which identifies whether a point in the latent vector will be swapped with the noised original latent vector at that timestep. We repeat this process for all the selected timesteps. 
        </p>
        </div>
        <!-- Architecture Diagram -->
        <div class="architecture-diagram">
            <img src="./static/Stable-diffusion.png" alt="Architecture Diagram" style="max-width: 70%; height: auto; margin-top: 20px;">
            <p>Figure 2: Architecture Diagram of Text-to-Image Synthesis with Dynamic Masking</p>
        </div>    
        <!-- Experiments / Results Section -->
        <h2>Experiments</h2>
        <div class="related-works">
        <p>For our experiments, we conducted text-to-image synthesis using both a baseline stable diffusion model and our masked model, with the aim of generating visually coherent images from textual prompts. We utilized the <b>Common Objects in Context (COCO) dataset</b> to create our database of reference images. The baseline model was configured with hyperparameters optimized for stability and quality, including a configuration scale of 8, a sampler of "ddpm," 50 inference steps, a seed value of 42, and a diffusion strength of 0.9. Among 50 timestamps, we selected the 5th, 10th and 15th timestamp for dynamic mask calculation and superimposed it over the predicted latent vector for those selected timestamps. 
            Dynamic map calculation was performed by calculating the attention scores for each word in the prompt and then normalizing the scores to obtain the mask. The threshold for the mask was set to  <b>mean of the cumulative map</b> (earlier 0.0002 for older results) and the regions with attention scores above the threshold were masked. In short we identify the less attended regions in the image and replace them with the orginal image to preserve the context.
            We evaluated our models across various prompts, ranging from simple descriptions like "Dog Sitting" to more complex scenarios such as "A Lion with Sunglasses." Each experiment involved synthesizing images based on the prompt using both the baseline model and our masked model. Additionally, we utilized an image of a "Dog Sitting in Grass" as a reference image to provide context for the synthesis process. 
            In our results, we observed the effectiveness of our masked model in preserving context and blending images compared to the baseline model. The structured integration of dynamic masking techniques with diffusion models allowed for enhanced accuracy in drawing subjects from input text onto images, ensuring seamless blending with the original image's visual features and background. Through meticulous experimentation and optimization, we aimed to validate the effectiveness of our approach in improving sample quality and providing greater user control in the generation process.
            Example of one qualitative result is shown below.
        </p>
        </div>

        <div class="output">
            <table>
                <tr>
                    <th>Prompt</th>
                    <th>Reference Image</th>
                    <th>Baseline Model: Stable Diffusion</th>
                    <th>Our Model</th>
                </tr>
                <tr>
                    <td>A dog with Sunglasses</td>
                    <td><img src="./static/reference.jpeg" alt="Reference Dog with Sunglasses" class="experiment-image"></td>
                    <td>
                        <img src="./static/dog-sunglasses-baseline.png" alt="Baseline Dog with Sunglasses" class="experiment-image">
                        <p>FID Score: 30.9550</p>
                    </td>
                    <td>
                        <img src="./static/dog-sunglasses-our.png" alt="Our Model Dog with Sunglasses" class="experiment-image">
                        <p>FID Score: 3.1471</p>
                    </td>
                </tr>
                <tr>
                    <td>A lion with Sunglasses</td>
                    <td><img src="./static/reference.jpeg" alt="Reference Dog with Sunglasses" class="experiment-image"></td>
                    <td>
                        <img src="./static/lion-sunglasses-baseline.png" alt="Baseline Dog with Sunglasses" class="experiment-image">
                        <p>FID Score: 5.7031</p>
                    </td>
                    <td>
                        <img src="./static/lion-sunglasses-our.png" alt="Our Model Dog with Sunglasses" class="experiment-image">
                        <p>FID Score: 1.7544</p>
                    </td>
                </tr>
                <tr>
                    <td>Horse running.</td>
                    <td><img src="./static/reference.jpeg" alt="Reference Dog with Sunglasses" class="experiment-image"></td>
                    <td>
                        <img src="./static/horse-running-baseline.png" alt="Baseline Dog with Sunglasses" class="experiment-image">
                        <p>FID Score: 40.7026</p>
                    </td>
                    <td>
                        <img src="./static/horse-running-our.png" alt="Our Model Dog with Sunglasses" class="experiment-image">
                        <p>FID Score: 5.7273</p>
                    </td>
                </tr>
            </table>

            <p> In the above table FID stands for Frechet Inception Distance, and is used to evaluate the quality of generated images. A lower FID score indicates a closer match to the original image. </p>
            
        </div>

        <h2>Results</h2>
        <div class="related-works">
            <p>Our model was able to generate images that were more coherent with the reference image and prompt compared to the baseline model. The images generated by our model had lower Frechet Inception Distance (FID) scores, indicating a closer match to the original image. 
            
            Other metrics used to evaluate the results were Peak Signal Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM), and in both cases the results for images for our model were higher than that of Stable Diffusion. A higher PSNR value indicates that the reconstructed image by our model, is of higher fidelity compared to the reference, and preserves the details and the quality of the original image better than the Stable Diffusion Model. A SSIM score measures how similar a given image is to a reference image, in terms of texture, luminance and contrast, and the higher values compared to Stable Diffusion results indicate that our model does a better job at maintaining structural properties and visual similarities from the original image, which is crucial for seamless blending of the existing background without distorting the existing integrity of the scene.

            The following charts show the results for each evaluation metric, as compared to the baseline Stable Diffusion model.
                
            </p>
        </div>

        <!-- Add your plot of FID scores, PSNR, and SSIM here -->
        <div>
            <!-- Insert your plot here -->
            <!-- You can use HTML <img> tag to display the plot -->
            <img src="./static/FID_Score_comparison.png" alt="Plot of FID scores, PSNR, and SSIM">
            <img src="./static/psnr_scores.png" alt="Plot of FID scores, PSNR, and SSIM">
            <img src="./static/ssi_scores.png" alt="Plot of FID scores, PSNR, and SSIM">
        </div>

        <div>
            The following table presents the results of our experiments for various prompts, comparing the baseline Stable Diffusion Model model with our masked model.
        </div>
        <!-- Add screenshots of image generations by stable diffusion and your model -->
        <div>
            <h3>Generated images:</h3>
            <!-- <hr style="margin-left: -8em; width: calc(100% + 16em); border-top: 2px solid #ddd;"> -->

            <div style="margin-top: 2em; position: relative;">
                <h4 style="margin-left: -8em;  position: relative; z-index: 50;margin-bottom: 3em;">Image Generations by Baseline Model (Stable Diffusion)</h4>
            
                <div style="display: flex; align-items: center; margin-left: -16em; margin-bottom: -3em; position: relative; z-index: 60;">
                    <div style="flex: 1; text-align: center;">
                        <p style="font-weight: bold; margin-left: -16em;">Base Image</p>
                    </div>
                    <div style="flex: 1; text-align: center;">
                        <p style="font-weight: bold;">Generated Image</p>
                    </div>
                </div>
                <!-- Insert your screenshot of image generations by baseline model here -->
                <img src="./static/table_gen.png" style="margin-left: -16em; position: relative; z-index: 40;" alt="Screenshot of image generations by Baseline Model">
            </div>
            <div style="margin-top: 2em; position: relative;">
                <h4 style="margin-left: -8em;  position: relative; z-index: 50;margin-bottom: 3em;">Image Generations by Our Model</h4>
            
                <div style="display: flex; align-items: center; margin-left: -16em; margin-bottom: -3em; position: relative; z-index: 60;">
                    <div style="flex: 1; text-align: center;">
                        <p style="font-weight: bold; margin-left: -16em;">Base Image</p>
                    </div>
                    <div style="flex: 1; text-align: center;">
                        <p style="font-weight: bold;">Generated Image</p>
                    </div>
                </div>
                <!-- Insert your screenshot of image generations by baseline model here -->
                <img src="./static/table.png" style="margin-left: -16em; position: relative; z-index: 40;" alt="Screenshot of image generations by Our Model">
            </div>
        </div>

        <div id="stableImageTable">
            <h3>Stable Model Images</h3>
          </div>
          <div id="baselineImageTable">
            <h3>Baseline Model Images</h3>
          </div>
          
        <script src="script.js"></script>
        <style>
            .image-table {
            max-width: 100%; /* Limit the width to the width of the screen */
            }

            .image-row {
            display: flex;
            justify-content: center;
            align-items: stretch; /* Align items in a way that they stretch to fill the container */
            margin-bottom: 20px; /* Adds space between the rows */
            }

            .image-row.header .prompt-title, .image-row .image-cell {
            flex-grow: 1; /* This will allow each cell to grow equally */
            flex-basis: 0; /* This prevents the flex items from growing beyond their content size */
            text-align: center;
            }

            .base-image-cell, .image-cell {
            padding: 10px; /* Add padding for some spacing around images */
            }

            .image-cell img {
            width: 100%; /* This will ensure the image takes the full width of the cell */
            height: auto;
            display: block;
            }

            .scores {
            text-align: center;
            font-size: 0.8em;
            }

            .prompt-title {
            font-weight: bold;
            }


        </style>

        <!-- What’s Next Section -->
        <!-- <h2>What’s Next</h2>
        <p>Our plan until the final project due date involves refining our masking technique, exploring additional datasets for evaluation, and conducting thorough experiments to validate our approach's effectiveness. We will also compare our results against baselines to assess performance improvements.</p> -->
        <!-- Task list indicating each step planned and anticipated completion date -->
        <!-- <table>
            <tr>
                <th>Task</th>
                <th>Anticipated Completion Date</th>
            </tr>
            <tr>
                <td>Refine and Improve masking technique to incorporate complex scenerios</td>
                <td>April 10, 2024</td>
            </tr>
            <tr>
                <td>Ablation Studies to identify better hyperparameters</td>
                <td>April 15, 2024</td>
            </tr>
            <tr>
                <td>Conduct further experiments</td>
                <td>April 20, 2024</td>
            </tr>
        </table> -->

		<h2 style="margin-top: 20px;">Discussion </h2>
        <div class="related-works">
            <p style="text-align: justify; margin-bottom: 20px;">We were successful in generating images that showed significant correlation with reference images when compared to those produced by another model without reference. Through our approach, we learned how to effectively distinguish areas of low and high attention that correspond with a text prompt. Replacing only the low attention areas with the reference allowed us to both satisfy the expected scenario that the prompt implied and maintain the original environment that was desired.</p>
            <p style="text-align: justify;">In the future, we would firstly attempt to achieve higher FID, PSNR, and SSISM scores, which would ensure better consistency in preserving the referenced backgrounds. We could also improve believability by adjusting how the exact position, size, and lighting for new subjects is determined when they are integrated with the background.</p>
        </div>
        
        <h2 style="margin-top: 20px;">Challenges Faced</h2>
        <div class="related-works">
            <p style="text-align: justify; margin-bottom: 20px;">Throughout the development and experimentation phase, we encountered several challenges that influenced the refinement of our approach. Below are the key challenges we faced:</p>
            <ol>
                <li><strong>Handling Multiple Subjects:</strong> When dealing with prompts involving multiple subjects, such as "A dog and a cat playing," the results were often affected by the overlap of masks. Integrating multiple subjects seamlessly into the image generation process remains an ongoing challenge.</li>
                <li><strong>Threshold Determination:</strong> Setting the threshold for the dynamic mask proved to be challenging and required extensive ablation studies. Finding the optimal threshold that balances preserving context while avoiding over-masking or under-masking regions remains a nuanced task.</li>
                <li><strong>Computational Complexity:</strong> The model's computational complexity posed challenges, particularly during the denoising process for each timestamp. Each denoising step requires significant computational resources, leading to slower image generation times.</li>
                <li><strong>Evaluation Overhead:</strong> Conducting thorough evaluations on the generated images posed logistical challenges. Generating multiple images for different prompts and reference images is a time-consuming process, making comprehensive evaluation a resource-intensive task.</li>
            </ol>
            <br/>
            <p style="text-align: justify;">Addressing these challenges will be crucial for further refining our approach and making it more practical for real-world applications. Strategies such as optimizing computational efficiency, refining masking techniques for multiple subjects, and streamlining the evaluation process will be key areas of focus for future research and development efforts.</p>
        </div>



        <!-- Team Contributions Section -->
        <div class="team-contributions">
            <h2>Team Contributions</h2>
            <table>
                <tr>
                    <th>Team Member</th>
                    <th>Contributions</th>
                </tr>
                <tr>
                    <td>Kshitij Pathania</td>
                    <td>Implemented dynamic masking technique, conducted experiments</td>
                </tr>
                <tr>
                    <td>Brandon Colbert</td>
                    <td>Research on related works, assisted in experimentation</td>
                </tr>
                <tr>
                    <td>Mathan Mahendran</td>
                    <td>Dataset exploration, documentation</td>
                </tr>
            </table>
        </div>
       
        <h3>References</h3>
        <ol>
        <li>
            <p>R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, "High-Resolution Image Synthesis with Latent Diffusion Models," 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Jun. 2022. DOI: 10.1109/cvpr52688.2022.01042</p>
        </li>
        <li>
            <p>H. Chefer, Y. Alaluf, Y. Vinker, L. Wolf, and D. Cohen-Or, "Attend-and-Excite: Attention-based semantic guidance for text-to-image diffusion models," ACM Transactions on Graphics, vol. 42, no. 4, pp. 1–10, Jul. 2023. DOI: 10.1145/3592116</p>
        </li>
        <li>
            <p>K. He et al., "Masked autoencoders are scalable vision learners," 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Jun. 2022. DOI: 10.1109/cvpr52688.2022.01553</p>
        </li>
        <li>
            <p>U. Jamil, "HKPROJ/Pytorch-stable-diffusion: Stable diffusion implemented from scratch in PyTorch," GitHub, https://github.com/hkproj/pytorch-stable-diffusion (accessed Feb. 21, 2024).</p>
        </li>
        <li>
            <p>"CompVis/stable-diffusion · hugging face," CompVis/stable-diffusion · Hugging Face, https://huggingface.co/CompVis/stable-diffusion (accessed Feb. 21, 2024).</p>
        </li>
        <li>
            <p>Create high-quality images with stable diffusion models and deploy ..., https://aws.amazon.com/blogs/machine-learning/create-high-quality-images-with-stable-diffusion-models-and-deploy-them-cost-efficiently-with-amazon-sagemaker/ (accessed Feb. 22, 2024).</p>
        </li>
        </ol>

    </div>
</body>

</html>
