<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Text-to-Image Synthesis with Dynamic Masking</title>
    <style>
        * {
            border: 0;
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            padding: 20px;
            margin: 0 auto;
            padding: 0;
        }

        .navbar {
            background-color: #333;
            overflow: hidden;
            display: flex;
            justify-content: center; /* Center the content */
            align-items: center;
            width: 100%;
            padding: 20px 0; /* Add vertical padding */
        }

        .navbar-center {
            text-align: center; /* Center-align the inner content */
        }

        .navbar-title {
            color: white;
            font-size: xx-large;
            margin-bottom: 10px; /* Space between title and names */
        }

        .navbar-info {
            color: white;
            margin-bottom: 10px; /* Space between names and GitHub button */
        }

        .github-btn {
            background-color: #4CAF50;
            color: white;
            padding: 10px 15px;
            text-decoration: none;
            border-radius: 5px;
            font-size: 14px;
        }

        .github-btn:hover {
            background-color: #45a049;
        }

        .container {
            max-width: 1000px;
            margin: 0 auto;
        }

        h1, h2 {
            margin-bottom: 20px;
            margin-top: 20px;
        }

        a {
            display: inline-block;
            margin-top: 20px;
            padding: 10px 15px;
            background-color: #007BFF;
            color: #FFF;
            text-decoration: none;
            border-radius: 5px;
        }

        .dropdown {
            margin: 20px 0;
        }

        .dropdown select {
            display: block;
            width: 100%;
            padding: 10px;
            font-size: 16px;
            border: 1px solid #ddd;
            border-radius: 5px;
        }

        .output {
            margin-top: 20px;
            padding: 15px;
            background-color: #f0f0f0;
            border: 1px solid #ddd;
            border-radius: 5px;
            margin-bottom: 5em;
        }
        .team-contributions {
            margin-top: 20px;
        }

        .team-contributions h2 {
            margin-bottom: 10px;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 10px;
            margin-bottom: 40px;
        }

        table, th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }

        th {
            background-color: #f2f2f2;
        }
        .related-works {
            text-align: justify;
        }

        .related-works p {
            display: inline-block;
            width: 100%;
        }

    </style>
</head>

<body>
    <nav class="navbar">
        <div class="navbar-center">
            <div class="navbar-title">
                Text-to-Image Synthesis with Dynamic Masking
            </div>
            <div class="navbar-info">
                Kshitij Pathania, Brandon Colbert, Mathan Mahendran
            </div>
            <!-- Replace the href with your repository link -->
            <a href="https://github.com/CS-6476/Masked-Diffusion-Image-Synthesis" target="_blank" class="github-btn">GitHub Repository</a>
        </div>
    </nav>
    
    
    <div class="container">
        <h1>Text-to-Image Synthesis Project</h1>

        <!-- Problem Statement Section -->
        <h2>Problem Statement</h2>
        <p>We aim to develop an advanced text-to-image synthesis method by seamlessly integrating a dynamic masking technique with diffusion models. This approach seeks to enhance the accuracy of drawing subjects from input text onto an image while ensuring their seamless blending with the original image's visual features and background.</p>

        <h2>Related Works</h2>
        <div class="related-works">
            <p>The research landscape in high-resolution image synthesis and denoising with latent diffusion models encompasses several key contributions. At the forefront is the original paper on stable diffusion models for image synthesis, titled "High-Resolution Image Synthesis with Latent Diffusion Models" [1]. Building upon this foundation, "Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models" introduces attention mechanisms to ensure adequate focus on subjects within prompts, enhancing the quality of generated images [2].</p>
            <p>Meanwhile, "SinDDM: A Single Image Denoising Diffusion Model" proposes a user-guided approach to image denoising, leveraging regions of interest (ROIs) selected by users and inspired by prior hierarchical text-conditional image generation techniques [3]. In the realm of multimodal image inpainting, the "Uni-paint Framework" offers a unified solution utilizing pretrained diffusion models, albeit with the caveat of computational expense due to fine-tuning requirements [6]. Additionally, "SAT: Self-Attention Control for Diffusion Models Training and Improving Sample Quality of Diffusion Models Using Self-Attention Guidance" presents a method for self-attention control, enhancing image quality by selectively blurring unattended areas and iteratively denoising the image [4].</p>
            <p>These approaches collectively advance the understanding and application of diffusion models in image synthesis and denoising tasks, offering diverse strategies for improving sample quality and enhancing user control in the generation process. Moreover, efforts to implement Stable Diffusion from scratch in Python [5] contribute to the accessibility and reproducibility of these advancements, fostering further research and development in the field.</p>
        </div>
        <!-- Methods/Approach Section -->
        <h2>Methods/Approach</h2>
        <p>Our approach involves integrating dynamic masking techniques with diffusion models for text-to-image synthesis. We leverage cumulative attention scores derived from the text prompt to mask specific areas of the image during the synthesis process. This allows us to preserve contextual information while generating visually coherent images.</p>

        <!-- Experiments / Results Section -->
        <h2>Experiments / Results</h2>
        <p>We conducted experiments using the Common Objects in Context (COCO) dataset to validate our approach. While our initial results show promising outcomes in preserving context and blending images, further experimentation and optimization are required to achieve desired performance.</p>
        <div class="output">
            <!-- Insert qualitative result (e.g., a visual output of your system on an example image) here -->
        </div>

        <!-- What’s Next Section -->
        <h2>What’s Next</h2>
        <p>Our plan until the final project due date involves refining our masking technique, exploring additional datasets for evaluation, and conducting thorough experiments to validate our approach's effectiveness. We will also compare our results against baselines to assess performance improvements.</p>
        <!-- Task list indicating each step planned and anticipated completion date -->
        <table>
            <tr>
                <th>Task</th>
                <th>Anticipated Completion Date</th>
            </tr>
            <tr>
                <td>Refine masking technique</td>
                <td>April 10, 2024</td>
            </tr>
            <tr>
                <td>Explore additional datasets</td>
                <td>April 15, 2024</td>
            </tr>
            <tr>
                <td>Conduct experiments</td>
                <td>April 20, 2024</td>
            </tr>
        </table>

        <!-- Team Contributions Section -->
        <div class="team-contributions">
            <h2>Team Contributions</h2>
            <table>
                <tr>
                    <th>Team Member</th>
                    <th>Contributions</th>
                </tr>
                <tr>
                    <td>Kshitij Pathania</td>
                    <td>Implemented dynamic masking technique, conducted experiments</td>
                </tr>
                <tr>
                    <td>Brandon Colbert</td>
                    <td>Research on related works, assisted in experimentation</td>
                </tr>
                <tr>
                    <td>Mathan Mahendran</td>
                    <td>Dataset exploration, documentation</td>
                </tr>
            </table>
        </div>
       
        <h3>References</h3>
        <ol>
        <li>
            <p>R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, "High-Resolution Image Synthesis with Latent Diffusion Models," 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Jun. 2022. DOI: 10.1109/cvpr52688.2022.01042</p>
        </li>
        <li>
            <p>H. Chefer, Y. Alaluf, Y. Vinker, L. Wolf, and D. Cohen-Or, "Attend-and-Excite: Attention-based semantic guidance for text-to-image diffusion models," ACM Transactions on Graphics, vol. 42, no. 4, pp. 1–10, Jul. 2023. DOI: 10.1145/3592116</p>
        </li>
        <li>
            <p>K. He et al., "Masked autoencoders are scalable vision learners," 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Jun. 2022. DOI: 10.1109/cvpr52688.2022.01553</p>
        </li>
        <li>
            <p>U. Jamil, "HKPROJ/Pytorch-stable-diffusion: Stable diffusion implemented from scratch in PyTorch," GitHub, https://github.com/hkproj/pytorch-stable-diffusion (accessed Feb. 21, 2024).</p>
        </li>
        <li>
            <p>"CompVis/stable-diffusion · hugging face," CompVis/stable-diffusion · Hugging Face, https://huggingface.co/CompVis/stable-diffusion (accessed Feb. 21, 2024).</p>
        </li>
        <li>
            <p>Create high-quality images with stable diffusion models and deploy ..., https://aws.amazon.com/blogs/machine-learning/create-high-quality-images-with-stable-diffusion-models-and-deploy-them-cost-efficiently-with-amazon-sagemaker/ (accessed Feb. 22, 2024).</p>
        </li>
        </ol>

    </div>
</body>

</html>
